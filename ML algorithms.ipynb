{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a new series of \"going in depth on most used ML algorithms\" series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors \n",
    "\n",
    "--> In the simplest explanation : You are determined by your closest neighbors.  \n",
    "\n",
    "--> Does not learn parameters to make predictions (Non-parameteric model). \n",
    "\n",
    "Different from parameteric models such as Linear regression and Logistic regression for example. \n",
    "\n",
    "\n",
    "Its powerful because it can be used in regression and classification tasks.\n",
    "\n",
    "--> How KNN works : \n",
    "\n",
    "1- Find the K closest neighbors.\n",
    "\n",
    "* To make a prediction of a new data point we fist need to find its k nearest neighbors from the data set to find which data points are the closest neighbors we need to measure the distance between the new data points and all existing mid-points.\n",
    "\n",
    "* Common distance matrics: Euclidean distance & Cosine similarity. \n",
    "\n",
    "2- Use the neighbors for prediction. \n",
    "\n",
    "* If it is a regression task we will take the average of all its closests neighbors values and that will be the prediction.\n",
    "\n",
    "* If it is a classification task the prediction will be the majority of the neighbor's class. \n",
    "\n",
    "Because of its simplicity and powerfulness K is so commonly used sometimes you may not realize you are using it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Implementation \n",
    "\n",
    "--> There are two steps :\n",
    "\n",
    "1- Obtaining the data. \n",
    "\n",
    "2- Querying the nearest neighbors. \n",
    "\n",
    "For KNN the data is shared between training and prediction.\n",
    "It makes sense to package our implementation in a class so that functions in that class could share variables.\n",
    "\n",
    "In python it is a good coding practice to declare all instance variables in the init function before using them or updating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = None  \n",
    "        self.y = None \n",
    "\n",
    "    def train(self, x, y):\n",
    "        \"\"\"\n",
    "        inputs : (x) is the new data point that we want to predict values for\n",
    "                 the dimension of the data we assume x is a 2-D array \n",
    "                 with the first dimension being the number of data points \n",
    "                 and the second dimension is the number of features. \n",
    "\n",
    "                (y) for regression problems the label's \"y\" is an array \n",
    "                of floating numbers.\n",
    "\n",
    "                    for classification  problems it's an array of integers each represents a class \n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, x, k): \n",
    "        \"\"\"\n",
    "        input : (x) is the data point(s) that we want to classify its value.\n",
    "\n",
    "                (k) is the number of neighbors to query. \n",
    "\n",
    "\n",
    "        To find the (k) closest neighbors the idea is to first get the distnace from \n",
    "        the new data point to all the training data points and sort them in ascending order based on \n",
    "        the distance then we just need to select the top (k) we specify. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        distance_label = [\n",
    "            (self.distance(x, train_point), train_label) for train_point, train_label in zip(self.x, self.y)] \n",
    "            \n",
    "            # here we used tuples to store the distance and the observed label (y), so that we could easily sort a list of tuples in ascending order based on the distance. \n",
    "\n",
    "        neighbors = sorted(distance_label)[:k]\n",
    "\n",
    "        # if it's a regression problem we can take the mean of the labels from all K neighbors.\n",
    "        # \n",
    "        # if it's a classification problem we can count the majority of labels from its neighbors \n",
    "        # \n",
    "        # neighbors_labels = [label for dist, label in neighbors]\n",
    "        #\n",
    "        # return Counter(neighbor_labels).most_common()[0][0]\n",
    "        \n",
    "        return sum(label for _, label in neighbors) / k     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Space & Time complexity \n",
    "\n",
    "In the train function we simply create two variables (x) and (y) and point them to the input data,\n",
    "So the space and time complexity are O(1)\n",
    "\n",
    "In the predict function we have the time complexity and space O(MN)\n",
    "where the M is the data point and N is the features \n",
    "\n",
    "The most time consuming part comes in the sorting which gives O(M log(M)) this is the time comlexity of python sorting algorithm. \n",
    "\n",
    "So the totla complexity of the predict function is O(MN) + O(M log(M))\n",
    "\n",
    "assuming that the number log(M) is larger than log(n) because typically the number of trining points \n",
    "is much more than the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose K ? \n",
    "\n",
    "k is a hyperparamter  * Predetermined , you could use any positive number that you think makes the most sense so it's kind arbitrary.\n",
    "\n",
    "--> When K is too small predictions can be noisy.\n",
    "\n",
    "--> When K is too large prediction is averaged over too many data pints the result is not accurate either. \n",
    "\n",
    "* One simple approach is to use the square root of the number of data points --> k = np.sqrt(data_points)\n",
    "\n",
    "This is an empirical value you could use if for quick experiments, but for more rigorous approach we can use cross validation to select the optimal (K) for each specific data set \n",
    "\n",
    "The goal of the cross validation is to use training data to help us test the choices of hyperparameters in machine learning models.\n",
    "\n",
    "To do that we shuffle the training data and divide it into (n) equal sized partitions, then we pick a range of values we want to select from for the hyperparameter for each candidate we use (n - 1) partitions for training and the remining one partition for validation.\n",
    "\n",
    "We compute the validation error associated with each candidate and then we select the one with the minimum error.that is how we can get the optimum value of K. \n",
    "\n",
    "We could also take a more robust approach by repeating this exercise on different partitions meaning every partition has a chance to be a validation data set so at the end of the process we have (n) validation error associated with each candidate.\n",
    "\n",
    "The average of those error becomes the cross validation error of that candidate. Finally, we just need to pick the candidate with the lowest cross validation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logiatic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun fact : Logistic regression is NOT regression !\n",
    "\n",
    "It is a binary classification model.\n",
    "\n",
    "It is used to predict a binary outcome from a linear combination of variables. So there are two possible outcomes representing two different classes (0) and (1)\n",
    "\n",
    "<i>For example:</i> we can use logistic regression to classify if an email is a spam or not based on the subject and the body of that email. \n",
    "\n",
    "\n",
    "<b>How does the model classify data?</b>\n",
    "\n",
    "1- Compute probability in class \"1\".\n",
    "\n",
    "It uses a function to project the linear combination of all features into scores between 0 and 1 representing the probability of being in class \"1\"\n",
    "\n",
    "This function is called a <i>logistic function</i> or <i>sigmoid function</i>  ++> g(z) = (1 / 1 + np.exp(-z))\n",
    "\n",
    "Will map any value into a range 0 to 1. Postive numbers become higher probabilities and negative numbers become low ones.\n",
    "\n",
    "2- Predict classes based on probability & threshold.\n",
    "\n",
    "If the probability is larger than the threshold the prediction will be class (1) otherwise it's class (0).\n",
    "\n",
    "--> P(x,beta) = P(y=1| x,beta) = 1 - P(y=0 | x,beta)\n",
    "\n",
    "where (x) represent independent variables in machine learning we call them features.\n",
    "\n",
    "(Beta) are the parameters. (P(x,beta)) is the logistic function (g) of the linear combination of features.\n",
    "\n",
    "P(x,beta) = g(beta_0, + beta_1 * x_1 + .... + beta_n * x_n)\n",
    "\n",
    "So you see that we have introduced a bunch of betas we need to use them to predict the outcome, but betas are unknown how do we get them? \n",
    "\n",
    "typically we use a training data set to obtain betas.\n",
    "\n",
    "Say they are a total of (M) training data points each data point has (n independent varaibles) from (x_1 to x_n) and observed class (y) so there will be (n+1) betas. \n",
    "\n",
    "We'll use this training process to obtain the values of all betas \n",
    "\n",
    "A method called <b>Maximum likelihood estimation</b> is often used to get betas specifically we use\n",
    "betas, X, and y to formulate the likelihood of getting the observed class then we obtain betas to maximize the likelihood. In other words, we want to select betas that maximaize the probability of observing the data we observe, so let's first write the likelihood of getting the observed class (Yi) for each training data point we have a vector of features (Xi)\n",
    "\n",
    "the probability of that class would be either p  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Principal Component Analysis (PCA)\n",
    "\n",
    "> PCA is a powerful technique for dimentionality reduction and data visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class PCA:\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None \n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        # center the data \n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        \n",
    "        X = X - self.mean\n",
    "        \n",
    "        # compute the covarience matrix \n",
    "        \n",
    "        cov = np.cov(X, rowvar = False)\n",
    "        \n",
    "        # compute the eigenvalues and eigenvectors of the covarience matrix \n",
    "        \n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "        \n",
    "        # sort the eigenvalues and eigenvectors of the covarience matrix \n",
    "        \n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        \n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # sort the first n_components eigenvectors as the principal components \n",
    "        self.components = eigenvectors[:,: self.n_components]\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = X - self.mean\n",
    "        \n",
    "        # project the data onto the principal components \n",
    "        X_transformed = np.dot(X, self.components)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.datasets import load_wine, load_iris \n",
    "\n",
    "wine_df = load_wine()\n",
    "\n",
    "X, y = wine_df['data'], wine_df['target']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plt.scatter(X[:, 0], X[:, 2], c = y)\n",
    "\n",
    "plt.legend(handles = plot.legend_elements()[0],\n",
    "           labels = list(wine_df['target_names']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pl.scatter(X_transformed[:,0], X_transformed[:,1], c = y)\n",
    "\n",
    "plt.legend(handles = plot.legend_elements()[0],\n",
    "           labels = list(wine_df['target_names']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "\n",
    "X_normalized = scaler.transform(X)\n",
    "\n",
    "pca.fit(X_normalized)\n",
    "\n",
    "X_transformed = pca.transform(X_normalized)\n",
    "\n",
    "plot = pl.scatter(X_normalized[:,0], X_normalized:,1], c = y)\n",
    "\n",
    "plt.legend(handles = plot.legend_elements()[0],\n",
    "           labels = list(wine_df['target_names']))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92cc9fca99d4c0a71238c8b9c2d26e770b13d33a798981924d8899af005c99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
