{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a new series of \"going in depth on most used ML algorithms\" series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors \n",
    "\n",
    "--> In the simplest explanation : You are determined by your closest neighbors.  \n",
    "\n",
    "--> Does not learn parameters to make predictions (Non-parameteric model). \n",
    "\n",
    "Different from parameteric models such as Linear regression and Logistic regression for example. \n",
    "\n",
    "\n",
    "Its powerful because it can be used in regression and classification tasks.\n",
    "\n",
    "--> How KNN works : \n",
    "\n",
    "1- Find the K closest neighbors.\n",
    "\n",
    "* To make a prediction of a new data point we fist need to find its k nearest neighbors from the data set to find which data points are the closest neighbors we need to measure the distance between the new data points and all existing mid-points.\n",
    "\n",
    "* Common distance matrics: Euclidean distance & Cosine similarity. \n",
    "\n",
    "2- Use the neighbors for prediction. \n",
    "\n",
    "* If it is a regression task we will take the average of all its closests neighbors values and that will be the prediction.\n",
    "\n",
    "* If it is a classification task the prediction will be the majority of the neighbor's class. \n",
    "\n",
    "Because of its simplicity and powerfulness K is so commonly used sometimes you may not realize you are using it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Implementation \n",
    "\n",
    "--> There are two steps :\n",
    "\n",
    "1- Obtaining the data. \n",
    "\n",
    "2- Querying the nearest neighbors. \n",
    "\n",
    "For KNN the data is shared between training and prediction.\n",
    "It makes sense to package our implementation in a class so that functions in that class could share variables.\n",
    "\n",
    "In python it is a good coding practice to declare all instance variables in the init function before using them or updating the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = None  \n",
    "        self.y = None \n",
    "\n",
    "    def train(self, x, y):\n",
    "        \"\"\"\n",
    "        inputs : (x) is the new data point that we want to predict values for\n",
    "                 the dimension of the data we assume x is a 2-D array \n",
    "                 with the first dimension being the number of data points \n",
    "                 and the second dimension is the number of features. \n",
    "\n",
    "                (y) for regression problems the label's \"y\" is an array \n",
    "                of floating numbers.\n",
    "\n",
    "                    for classification  problems it's an array of integers each represents a class \n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x \n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, x, k): \n",
    "        \"\"\"\n",
    "        input : (x) is the data point(s) that we want to classify its value.\n",
    "\n",
    "                (k) is the number of neighbors to query. \n",
    "\n",
    "\n",
    "        To find the (k) closest neighbors the idea is to first get the distnace from \n",
    "        the new data point to all the training data points and sort them in ascending order based on \n",
    "        the distance then we just need to select the top (k) we specify. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        distance_label = [\n",
    "            (self.distance(x, train_point), train_label) for train_point, train_label in zip(self.x, self.y)] \n",
    "            \n",
    "            # here we used tuples to store the distance and the observed label (y), so that we could easily sort a list of tuples in ascending order based on the distance. \n",
    "\n",
    "        neighbors = sorted(distance_label)[:k]\n",
    "\n",
    "        # if it's a regression problem we can take the mean of the labels from all K neighbors.\n",
    "        # \n",
    "        # if it's a classification problem we can count the majority of labels from its neighbors \n",
    "        # \n",
    "        # neighbors_labels = [label for dist, label in neighbors]\n",
    "        #\n",
    "        # return Counter(neighbor_labels).most_common()[0][0]\n",
    "        \n",
    "        return sum(label for _, label in neighbors) / k     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Space & Time complexity \n",
    "\n",
    "In the train function we simply create two variables (x) and (y) and point them to the input data,\n",
    "So the space and time complexity are O(1)\n",
    "\n",
    "In the predict function we have the time complexity and space O(MN)\n",
    "where the M is the data point and N is the features \n",
    "\n",
    "The most time consuming part comes in the sorting which gives O(M log(M)) this is the time comlexity of python sorting algorithm. \n",
    "\n",
    "So the totla complexity of the predict function is O(MN) + O(M log(M))\n",
    "\n",
    "assuming that the number log(M) is larger than log(n) because typically the number of trining points \n",
    "is much more than the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose K ? \n",
    "\n",
    "k is a hyperparamter  * Predetermined , you could use any positive number that you think makes the most sense so it's kind arbitrary.\n",
    "\n",
    "--> When K is too small predictions can be noisy.\n",
    "\n",
    "--> When K is too large prediction is averaged over too many data pints the result is not accurate either. \n",
    "\n",
    "* One simple approach is to use the square root of the number of data points --> k = np.sqrt(data_points)\n",
    "\n",
    "This is an empirical value you could use if for quick experiments, but for more rigorous approach we can use cross validation to select the optimal (K) for each specific data set \n",
    "\n",
    "The goal of the cross validation is to use training data to help us test the choices of hyperparameters in machine learning models.\n",
    "\n",
    "To do that we shuffle the training data and divide it into (n) equal sized partitions, then we pick a range of values we want to select from for the hyperparameter for each candidate we use (n - 1) partitions for training and the remining one partition for validation.\n",
    "\n",
    "We compute the validation error associated with each candidate and then we select the one with the minimum error.that is how we can get the optimum value of K. \n",
    "\n",
    "We could also take a more robust approach by repeating this exercise on different partitions meaning every partition has a chance to be a validation data set so at the end of the process we have (n) validation error associated with each candidate.\n",
    "\n",
    "The average of those error becomes the cross validation error of that candidate. Finally, we just need to pick the candidate with the lowest cross validation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
